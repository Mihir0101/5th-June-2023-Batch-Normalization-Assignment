{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c08c43-e9fa-441e-88c6-128718365ac9",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d923e51-285d-4f8b-a7de-59199c1ddf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d62c1a-0a95-46a6-967c-0c311a39d2b4",
   "metadata": {},
   "source": [
    "## **Q1. Theory and Concepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b6485-afb9-4359-af32-7bb48b24fd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d8e554-97d1-4ecd-9a77-bc5f6f92786a",
   "metadata": {},
   "source": [
    "## 1. Explain the concept of batch normalization in the context of Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf7f30-4121-4594-b490-05f9c08613ba",
   "metadata": {},
   "source": [
    "-> Batch Normalization is used to improve the training process and performance of the artificial neural network by normalizing the input values to each layer.\n",
    "\n",
    "-> It is also used to solve some problems like internal covariate shift.\n",
    "\n",
    "* **There are two methods of applying batch normalization on data :**\n",
    "\n",
    "1. First normalization on input data then compute the neuron.\n",
    "\n",
    "2. First compute the neuron and then apply normalization on the output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704129d1-a44e-4aae-97ce-1f18fdededaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5eec5d2-c603-4802-a3d2-063ad470a122",
   "metadata": {},
   "source": [
    "## 2. Describe the benefits of using batch normalization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdf48b-eec7-4478-b1bd-8236e724513a",
   "metadata": {},
   "source": [
    "1. **Reduces the Internal Covariate Shift**\n",
    "\n",
    "-> It helps to stabalize the distribution by applying normalization on it.\n",
    "\n",
    "2. **Gradient Flow**\n",
    "\n",
    "-> It maintain the gradient flow, that helps to prevent issues like vanishing gradient.\n",
    "\n",
    "3. **Accelerate Convergence**\n",
    "\n",
    "-> It allows higher learning rate, so we can reduce the number of epochs and can accelerate the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2e09e-c71d-428c-b808-8195f22208c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3610ada-8599-4c33-a5bb-a6ebf060d846",
   "metadata": {},
   "source": [
    "## 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949c6f1-ad13-4778-ba7b-f020c263a747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76adfb19-d809-4db0-9696-9b6caef56df3",
   "metadata": {},
   "source": [
    "* **Normalization Steps**\n",
    "\n",
    "**Step-1 : Compute batch statistics**\n",
    "\n",
    "mean = 1/n * sum(Xi)\n",
    "\n",
    "stddev = np.sqrt(sum(Xi - mean) / (n - 1))\n",
    "\n",
    "**Step-2 : Normalize Activations**\n",
    "\n",
    "Standard Normalization(X) : (Xi - mean) / stddev\n",
    "\n",
    "* **Learnable Parameters**\n",
    "\n",
    "-> Apply linear transformation on normalized activation by the learnable parameters 𝛾(scale) and 𝛽(shift) :\n",
    " \n",
    "   Yij = 𝛾Xi + 𝛽, where  \n",
    "   \n",
    "   𝛾 : Controls the scaling of the normalized activation\n",
    "   \n",
    "   𝛽 : Controls the shifting of the normalized activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2a890-e80f-46e1-bfb9-a134c3908d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d3a420-ce7b-4c3c-a6fa-c102b5a4b6b7",
   "metadata": {},
   "source": [
    "## **Q2. Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203475e9-8c5e-496a-8b92-1f5b9f46e9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1\n",
      "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.10.0\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras>=3.2.0\n",
      "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.18,>=2.17\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.28.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.1)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.13.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, optree, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.65.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.0 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 rich-13.7.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 typing-extensions-4.12.2 werkzeug-3.0.3 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be080cc3-857a-464b-b795-c47080ca1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6ffde3-360e-45dc-be94-bf658b801269",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be28524f-0d2d-4bb0-b1aa-da3c5acbd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc421340-3bcc-44cc-9db2-bfa437af49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test / 255\n",
    "x_valid,x_train = x_train[:5000]/255 , x_train[5000:]/255\n",
    "y_valid,y_train = y_train[:5000] , y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10342279-aaf2-4682-ab28-d2e27db8efdc",
   "metadata": {},
   "source": [
    "## Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1143b6-29e7-4878-9e62-cf145d158929",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.Dense(300,activation='relu'),\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.Dense(10,activation='softmax'),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f793fff7-8eeb-4dd1-af6e-909641d3c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1e3297-541a-479f-a8c7-c16ba2250250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae29a91-74a7-4992-a2ae-7059db8dbd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8862 - loss: 0.3748 - val_accuracy: 0.9706 - val_loss: 0.0971\n",
      "Epoch 2/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0870 - val_accuracy: 0.9716 - val_loss: 0.0938\n",
      "Epoch 3/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9822 - loss: 0.0563 - val_accuracy: 0.9802 - val_loss: 0.0682\n",
      "Epoch 4/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9860 - loss: 0.0426 - val_accuracy: 0.9772 - val_loss: 0.0783\n",
      "Epoch 5/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0320 - val_accuracy: 0.9768 - val_loss: 0.0809\n",
      "Epoch 6/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0249 - val_accuracy: 0.9778 - val_loss: 0.0754\n",
      "Epoch 7/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9938 - loss: 0.0194 - val_accuracy: 0.9820 - val_loss: 0.0750\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f74cf3-9edf-45e7-be6d-538e006c0611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f31127-8b3c-407a-9164-e8b2302d2f76",
   "metadata": {},
   "source": [
    "## With Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4083b557-9fd2-4a74-b972-da47b71b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d3084fd-02e3-4679-ba0f-7256ac55b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_bn = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d95d45f-894b-4e38-b3b6-404324492cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_bn.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3530f4bc-0080-4008-b054-cf9e50ae88b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.8931 - loss: 0.3446 - val_accuracy: 0.9656 - val_loss: 0.1096\n",
      "Epoch 2/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.1078 - val_accuracy: 0.9710 - val_loss: 0.1027\n",
      "Epoch 3/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9724 - loss: 0.0827 - val_accuracy: 0.9756 - val_loss: 0.0890\n",
      "Epoch 4/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.0626 - val_accuracy: 0.9764 - val_loss: 0.0872\n",
      "Epoch 5/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9834 - loss: 0.0510 - val_accuracy: 0.9784 - val_loss: 0.0953\n",
      "Epoch 6/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9863 - loss: 0.0427 - val_accuracy: 0.9784 - val_loss: 0.0963\n",
      "Epoch 7/7\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.0373 - val_accuracy: 0.9774 - val_loss: 0.0995\n"
     ]
    }
   ],
   "source": [
    "MODEL_WITH_BN = model_with_bn.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1f44c-8d7e-477f-8c8a-00081154f3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c271428-10d9-4842-9001-1c7d58b25b70",
   "metadata": {},
   "source": [
    "**Model without Batch Normalization : Accuracy = 0.9938 & Loss = 0.0194** \n",
    "\n",
    "**Model with Batch Normalization : Accuracy = 0.9885 & Loss = 0.0373**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536c5dd-0a0e-45ba-8bd3-becd9a0e7659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41a63279-6fd3-4e85-bdd2-a896620ec917",
   "metadata": {},
   "source": [
    "## **Q3. Experiments and Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a6395-bec0-4bd5-a94a-a189b8273f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc7c83bd-4fe3-4817-856c-f9bcb0c957dd",
   "metadata": {},
   "source": [
    "## 1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867d173-e5c1-4091-9659-9cd9d96621bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf9a007-d218-46f4-a270-909010ff9588",
   "metadata": {},
   "source": [
    "* Batch Size = 33 | Accuracy = 0.9873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5afb62d-db66-41e3-94c3-e15d4e9dbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_with_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b463a88e-7add-4610-a74d-e3da5bca1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8890 - loss: 0.3505 - val_accuracy: 0.9662 - val_loss: 0.1118\n",
      "Epoch 2/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9620 - loss: 0.1206 - val_accuracy: 0.9726 - val_loss: 0.0898\n",
      "Epoch 3/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9758 - loss: 0.0760 - val_accuracy: 0.9720 - val_loss: 0.1042\n",
      "Epoch 4/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.0640 - val_accuracy: 0.9776 - val_loss: 0.0838\n",
      "Epoch 5/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.0511 - val_accuracy: 0.9754 - val_loss: 0.1000\n",
      "Epoch 6/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0439 - val_accuracy: 0.9732 - val_loss: 0.0940\n",
      "Epoch 7/7\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0391 - val_accuracy: 0.9800 - val_loss: 0.0807\n"
     ]
    }
   ],
   "source": [
    "LAYERS = [\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "        ]\n",
    "\n",
    "model_33 = tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "model_33.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "MODEL = model_33.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=7,batch_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bbd5d0e-8a37-40f6-a67e-f23cad336c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374346e-e3f5-4414-aaf4-b5a03e3df328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92dbaf39-7c67-445a-816e-6bbcf60a58af",
   "metadata": {},
   "source": [
    "* Batch Size = 333 | Accuracy = 0.9976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df9c4f-465c-4475-8397-d150aecbac8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21274c2-8eb7-404a-8400-f49ce297f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8346 - loss: 0.5533 - val_accuracy: 0.9488 - val_loss: 0.2543\n",
      "Epoch 2/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9714 - loss: 0.1024 - val_accuracy: 0.9716 - val_loss: 0.1047\n",
      "Epoch 3/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9844 - loss: 0.0579 - val_accuracy: 0.9764 - val_loss: 0.0804\n",
      "Epoch 4/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9914 - loss: 0.0346 - val_accuracy: 0.9788 - val_loss: 0.0787\n",
      "Epoch 5/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9945 - loss: 0.0232 - val_accuracy: 0.9788 - val_loss: 0.0798\n",
      "Epoch 6/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9967 - loss: 0.0144 - val_accuracy: 0.9768 - val_loss: 0.0806\n",
      "Epoch 7/7\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9976 - loss: 0.0113 - val_accuracy: 0.9778 - val_loss: 0.0831\n"
     ]
    }
   ],
   "source": [
    "LAYERS = [\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "        ]\n",
    "\n",
    "model_333 = tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "model_333.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "MODEL = model_333.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=7,batch_size=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aa94941-7423-4ea2-b1c6-8151f028a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf775d7-d219-4780-bebe-ed943543cd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47997cc-c5e5-4959-bf5f-806d654a1c3e",
   "metadata": {},
   "source": [
    "* Batch Size = 3333 | Accuracy = 0.9792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "385d50cc-833b-45b3-b49b-c4ebee29b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.5724 - loss: 1.3792 - val_accuracy: 0.7076 - val_loss: 1.3246\n",
      "Epoch 2/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9158 - loss: 0.2951 - val_accuracy: 0.8514 - val_loss: 0.9724\n",
      "Epoch 3/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9406 - loss: 0.2058 - val_accuracy: 0.8988 - val_loss: 0.7819\n",
      "Epoch 4/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9579 - loss: 0.1534 - val_accuracy: 0.9072 - val_loss: 0.6635\n",
      "Epoch 5/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9648 - loss: 0.1281 - val_accuracy: 0.9138 - val_loss: 0.5678\n",
      "Epoch 6/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9727 - loss: 0.1045 - val_accuracy: 0.9076 - val_loss: 0.5023\n",
      "Epoch 7/7\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9792 - loss: 0.0840 - val_accuracy: 0.8850 - val_loss: 0.4648\n"
     ]
    }
   ],
   "source": [
    "LAYERS = [\n",
    "        tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "        ]\n",
    "\n",
    "model_3333 = tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "model_3333.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "MODEL = model_3333.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=7,batch_size=3333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daa64262-885a-41c7-bb9c-a35d01d46a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_3333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da0123-306d-482e-bf4a-7d7689a3b10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d12ce8b-1f29-43d0-ab2a-d1fa9fda89ed",
   "metadata": {},
   "source": [
    "## 2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd255df0-cb79-4e5f-a878-4e3a0a4104c1",
   "metadata": {},
   "source": [
    "* Advantages\n",
    "\n",
    "1. **Internel Covariate Shift Reduction** : By normalizing the distributed data batch normalization reduces the internal covariate shift which makes the training process more stable.\n",
    "\n",
    "2. **Faster Convergence** : Batch normalization allows higher learning rate which leads the faster convergence.\n",
    "\n",
    "3. **Overfitting** : It also helps to reduce the overfitting.\n",
    "\n",
    "* Disadvantages\n",
    "\n",
    "1. **Computational Overhead** : Batch Normalization add additional computational steps which increases the computational time and consumes more computational resources.\n",
    "\n",
    "2. **Small Batch Size** : It can lead more noisy estimation of mean and variance which can affect the effectiveness of the normalization.\n",
    "\n",
    "3. **** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad245d-b64e-4e72-b62b-225115079641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
